# -*- coding: utf-8 -*-
"""Spark-NAS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sfUhS3o0gcycqR58HcPF_dReXRpTC4F3
"""

!pip install hyperopt
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from hyperopt import STATUS_OK
X, y = make_classification(n_samples=1000, n_features=4,
                           n_informative=2, n_redundant=0,
                           random_state=0, shuffle=False)

from hyperopt import fmin, tpe, hp,  STATUS_OK, Trials

def objective(C):
    # Create a support vector classifier model
    clf = RandomForestClassifier(max_depth=C, random_state=0)
    
    # Use the cross-validation accuracy to compare the models' performance
    accuracy = cross_val_score(clf, X, y).mean()
    
    # Hyperopt tries to minimize the objective function. A higher accuracy value means a better model, so you must return the negative accuracy.
    return {'loss': -accuracy, 'status': STATUS_OK}

search_space = hp.lognormal('C', 0, 1.0)

algo=tpe.suggest

"""Single Node"""

argmin = fmin(
  fn=objective,
  space=search_space,
  algo=algo,
  max_evals=16)

"""Multi-node

"""

!pip install mlflow
from hyperopt import SparkTrials
import mlflow
spark_trials = SparkTrials()
 
with mlflow.start_run():
  argmin = fmin(
    fn=objective,
    space=search_space,
    algo=algo,
    max_evals=16,
    trials=spark_trials)